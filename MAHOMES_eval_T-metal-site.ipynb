{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAHOMES holdout T-metal-site evaluation\n",
    "This notebook trains an ML model using the metal ion site data-set and evaluates the model's predictions on the T-metal-site set. The models and scalers are saved during this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHello World !\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# scale features\n",
    "from sklearn import preprocessing\n",
    "from sklearn import impute\n",
    "# classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# scoring metrics\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "\n",
    "# custom scripts\n",
    "import sys\n",
    "sys.path.insert(0, \"%s\" % \"CV/\")\n",
    "import GetFeatureSet as GetFeatureSet\n",
    "\n",
    "# allow fancey printed strings \n",
    "# from https://stackoverflow.com/questions/8924173/how-do-i-print-bold-text-in-python/8930747\n",
    "class color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "print(color.BOLD + 'Hello World !' + color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used to produce a new saved version of MAHOMES (ModelsForMAHOMES/) incase \n",
    "# the gitHub pickles don't work (generated using python 3.6.10 and MacOS 10.15.7)\n",
    "save_models = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data-set and T-metal-sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAll features:\u001b[0m\n",
      "sites: 3981 \tcolumns: 485\n",
      "Set   Catalytic\n",
      "data  False        2636\n",
      "      True          829\n",
      "test  False         345\n",
      "      True          171\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sites = pd.read_csv(\"publication_sites/sites_calculated_features.txt\")\n",
    "sites = sites.set_index('SITE_ID',drop=True)\n",
    "\n",
    "# The following labels need to be changed after looking over literature (see Feehan, Franklin, Slusky 2021)\n",
    "change_site_labels = [\"5zb8_0\", \"6aci_0\", \"6oq7_0\", \"6pjv_1\", \"6q55_0\",\n",
    "                      \"6q55_2\", \"6rmg_0\", \"6rtg_0\", \"6rw0_0\", \"6v77_0\"]\n",
    "# The following sites are removed due to unkopwn correct labels (see Feehan, Franklin, Slusky 2021)\n",
    "sites.loc[sites.index.isin(change_site_labels), 'Catalytic']=True\n",
    "remove_sites = [\"6mf0_1\", \"6okh_0\", \"6qwo_0\", \"6r9n_0\"]\n",
    "sites=sites.loc[~sites.index.isin(remove_sites)]\n",
    "\n",
    "print(color.BOLD + \"All features:\" + color.END)\n",
    "print(\"sites: %s \\tcolumns: %s\"%(sites.shape[0], sites.shape[1]))\n",
    "sizes = sites.groupby([\"Set\", \"Catalytic\"]).size()\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAll scaled data-set features:\u001b[0m\n",
      "sites: 3465 \tcolumns: 484\n",
      "Catalytic\n",
      "False    2636\n",
      "True      829\n",
      "dtype: int64\n",
      "\u001b[1m\n",
      "All scaled T-metal-site features:\u001b[0m\n",
      "sites: 516 \tcolumns: 484\n",
      "Catalytic\n",
      "False    345\n",
      "True     171\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def get_scaled_features():\n",
    "    # seperate the sets (only dataset will be used to set scaling)\n",
    "    data = sites.loc[sites.Set == \"data\"].copy()\n",
    "    Tsites = sites.loc[sites.Set == \"test\"].copy()\n",
    "\n",
    "    #split for scaling into categorical and not categorical\n",
    "    not_ctg_geom = (\"geom_gRMSD\", \"geom_MaxgRMSDDev\",\"geom_val\", \"geom_nVESCUM\",\"geom_AtomRMSD\", \"geom_AvgO\", \"geom_AvgN\", \"geom_AvgS\", \"geom_AvgOther\", \"geom_Charge\")\n",
    "    geom = [name for name in data if name.startswith(\"geom\")]\n",
    "    \n",
    "    ctg_data = [x for x in geom if not x in not_ctg_geom]\n",
    "    ctg_data.extend([\"Set\", 'Catalytic'])\n",
    "\n",
    "    ## scale cont. features\n",
    "    cont_scaler = preprocessing.RobustScaler(quantile_range=(20,80))\n",
    "    #Fit scaler to X, then transform it\n",
    "    data_nonctg = data[data.columns.difference(ctg_data)]#so that I can have columns\n",
    "    data_scaled = pd.DataFrame(cont_scaler.fit_transform(data_nonctg), columns=data_nonctg.columns, index=data_nonctg.index)\n",
    "    \n",
    "    #scale the test set based on the scale of the training set\n",
    "    Tsites_nonctg  = Tsites[Tsites.columns.difference(ctg_data)]\n",
    "    Tsites_scaled = pd.DataFrame(cont_scaler.transform(Tsites_nonctg), columns=Tsites_nonctg.columns, index=Tsites_nonctg.index)\n",
    "    \n",
    "    #replace continuous feature null values with mean\n",
    "    cont_imputer = impute.SimpleImputer(strategy=\"mean\")\n",
    "    data_scaled = pd.DataFrame(cont_imputer.fit_transform(data_scaled), columns=data_scaled.columns, index=data_scaled.index)\n",
    "    Tsites_scaled = pd.DataFrame(cont_imputer.transform(Tsites_scaled), columns=Tsites_scaled.columns, index=Tsites_scaled.index)\n",
    "    \n",
    "    if save_models==True:\n",
    "        joblib.dump(cont_scaler, \"ModelsForMAHOMES/ContVarScaler.pkl\")\n",
    "        joblib.dump(cont_imputer, \"ModelsForMAHOMES/ContVarImpute.pkl\")\n",
    "    \n",
    "    #remove groupID so that it also isn't MinMax scaled either\n",
    "    ctg_data.remove(\"Set\");ctg_data.remove(\"Catalytic\");\n",
    "    #transform categorical data to [0,1] interval\n",
    "    if len(data.columns.intersection(ctg_data)) > 0:\n",
    "        ctg_scaler = preprocessing.MinMaxScaler()\n",
    "        \n",
    "        # fit the scaler to the data-set (training) and scale\n",
    "        data_ctg = data[data.columns.intersection(ctg_data)]\n",
    "        data_ctg_scaled = pd.DataFrame(ctg_scaler.fit_transform(data_ctg), columns=data_ctg.columns, index=data_ctg.index)\n",
    "        \n",
    "        #scale the test set based on the scale of the training set\n",
    "        Tsites_ctg = Tsites[Tsites.columns.intersection(ctg_data)]\n",
    "        Tsites_ctg_scaled = pd.DataFrame(ctg_scaler.transform(Tsites_ctg), columns=Tsites_ctg.columns, index=Tsites_ctg.index) \n",
    "        \n",
    "        #replace categoric features null values with median value\n",
    "        ctg_imputer = impute.SimpleImputer(strategy=\"median\")\n",
    "        data_ctg_scaled = pd.DataFrame(ctg_imputer.fit_transform(data_ctg_scaled), columns=data_ctg_scaled.columns, index=data_ctg_scaled.index)\n",
    "        Tsites_ctg_scaled = pd.DataFrame(ctg_imputer.transform(Tsites_ctg_scaled), columns=Tsites_ctg_scaled.columns, index=Tsites_ctg_scaled.index)\n",
    "        \n",
    "        #concatenate the scaled categorical data to the robustly scaled data\n",
    "        data_scaled = pd.merge(data_scaled, data_ctg_scaled, left_index=True, right_index=True)\n",
    "        Tsites_scaled = pd.merge(Tsites_scaled, Tsites_ctg_scaled, left_index=True, right_index=True)\n",
    "        \n",
    "        if save_models==True:\n",
    "            joblib.dump(ctg_scaler, \"ModelsForMAHOMES/CtgVarScaler.pkl\")\n",
    "            joblib.dump(ctg_imputer, \"ModelsForMAHOMES/CtgVarImpute.pkl\")\n",
    "            \n",
    "    data_scaled = pd.merge(data_scaled, data['Catalytic'], left_index=True, right_index=True)\n",
    "    Tsites_scaled = pd.merge(Tsites_scaled, Tsites['Catalytic'], left_index=True, right_index=True)\n",
    "    \n",
    "    return(data_scaled, Tsites_scaled)\n",
    "\n",
    "data_scaled, Tsites_scaled = get_scaled_features()\n",
    "\n",
    "print(color.BOLD + \"All scaled data-set features:\" + color.END)\n",
    "print(\"sites: %s \\tcolumns: %s\"%(data_scaled.shape[0], data_scaled.shape[1]))\n",
    "print(data_scaled.groupby([\"Catalytic\"]).size())\n",
    "\n",
    "print(color.BOLD + \"\\nAll scaled T-metal-site features:\" + color.END)\n",
    "print(\"sites: %s \\tcolumns: %s\"%(Tsites_scaled.shape[0], Tsites_scaled.shape[1]))\n",
    "print(Tsites_scaled.groupby([\"Catalytic\"]).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performace using T-metal-sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returns relevent data-set data for training ML model\n",
    "def get_training_data(feature_set, random_seed):\n",
    "    ## random under sample data-set (1+:3-)\n",
    "    X_Cat = data_scaled[data_scaled['Catalytic']==True].copy()\n",
    "    X_nonCat = data_scaled[data_scaled['Catalytic']==False].copy()\n",
    "    X_nonCat = X_nonCat.sample(n=len(X_Cat)*3, axis=0, random_state=random_seed)\n",
    "    X_prep = X_Cat.append(X_nonCat)\n",
    "    \n",
    "    ## seperate target value\n",
    "    y = X_prep['Catalytic']; del X_prep['Catalytic']\n",
    "    \n",
    "    ## only return features in specific feature set\n",
    "    X = GetFeatureSet.feature_subset(X_prep, feature_set, noBSA=True)\n",
    "    \n",
    "    return(X, y)\n",
    "\n",
    "## number of iterations to improve reproducability\n",
    "num_rand_seeds = 10 # 10 provides 3 decimal level reproducability across my machines)\n",
    "def evaluate_model_with_Tsite(clf, feature_set):\n",
    "    ## prepare test-set\n",
    "    testX = Tsites_scaled.copy(); \n",
    "    testY = testX['Catalytic']; del testX['Catalytic']\n",
    "    testX = GetFeatureSet.feature_subset(testX, feature_set, noBSA=True)\n",
    "    \n",
    "    ## get multiple predictions for test-set w/ diff random seeds\n",
    "    test_site_preds = {'actual': pd.Series(testY, index=testX.index)}\n",
    "    for rand_seed in range(0,num_rand_seeds):\n",
    "        # get undersampled training data for feature set \n",
    "        X, y = get_training_data(feature_set, rand_seed)\n",
    "        print(\"random_seed = %s\"%(rand_seed), end=\"\\t\")\n",
    "        print(\"(num. training sites= %s (%s+ : %s-) \\tnum. features: %s)\"%(X.shape[0], len(y[y==True]),len(y[y==False]), X.shape[1]))\n",
    "        \n",
    "        ## train classifier and make test-set predictions\n",
    "        clf.set_params(random_state=rand_seed)\n",
    "        clf.fit(X, y)\n",
    "        test_preds = clf.predict(testX)\n",
    "        test_site_preds['prediction_%s'%(rand_seed)]= pd.Series(test_preds, index=testX.index)\n",
    "        if save_models==True:\n",
    "            joblib.dump(clf, \"ModelsForMAHOMES/MAHOMES%s.pkl\"%(rand_seed))\n",
    "        \n",
    "        ## output results for this random seed to get an idea of prediction variation levels\n",
    "        TN, FP, FN, TP = confusion_matrix(testY, test_preds).ravel()\n",
    "        mcc = matthews_corrcoef(testY, test_preds)\n",
    "        print(\"\\tTP=%s \\tTN=%s \\tFP=%s \\tFN=%s\"%(TP, TN, FP, FN))\n",
    "\n",
    "    ## calcualte the average of all random seed predictions\n",
    "    test_predictions = pd.DataFrame(test_site_preds)\n",
    "    test_predictions['prediction']=0\n",
    "    for rand_seed in range(0,num_rand_seeds):\n",
    "        test_predictions['prediction']+=test_predictions['prediction_%s'%(rand_seed)] \n",
    "    test_predictions['prediction']=test_predictions['prediction']/num_rand_seeds\n",
    "    \n",
    "    ## make final prediction\n",
    "    test_predictions['bool_pred']=False\n",
    "    test_predictions.loc[test_predictions['prediction']>=0.5, 'bool_pred']=True\n",
    "    \n",
    "    return(test_predictions)\n",
    "\n",
    "## return result metrics for final predictions\n",
    "def check_result_metrics(alg, feat_set, prediction_df):\n",
    "    mcc = matthews_corrcoef(prediction_df['actual'], prediction_df['bool_pred'])\n",
    "    TN, FP, FN, TP = confusion_matrix(prediction_df['actual'], prediction_df['bool_pred']).ravel()\n",
    "    \n",
    "    TPR=(TP/(TP+FN))*100\n",
    "    TNR=(TN/(TN+FP))*100\n",
    "    acc=((TP+TN)/(TP+TN+FP+FN))*100\n",
    "    Prec=(TP/(TP+FP))*100\n",
    "    return(pd.DataFrame([[alg, feat_set, acc, mcc, TPR, TNR, Prec]],\n",
    "        columns=['Algorithm', 'Feature Set', 'Accuracy', 'MCC', 'Recall', 'TrueNegRate', 'Precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_seed = 0\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=153 \tTN=333 \tFP=12 \tFN=18\n",
      "random_seed = 1\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=150 \tTN=333 \tFP=12 \tFN=21\n",
      "random_seed = 2\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=154 \tTN=329 \tFP=16 \tFN=17\n",
      "random_seed = 3\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=153 \tTN=332 \tFP=13 \tFN=18\n",
      "random_seed = 4\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=154 \tTN=331 \tFP=14 \tFN=17\n",
      "random_seed = 5\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=153 \tTN=330 \tFP=15 \tFN=18\n",
      "random_seed = 6\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=153 \tTN=331 \tFP=14 \tFN=18\n",
      "random_seed = 7\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=154 \tTN=331 \tFP=14 \tFN=17\n",
      "random_seed = 8\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=151 \tTN=333 \tFP=12 \tFN=20\n",
      "random_seed = 9\t(num. training sites= 3316 (829+ : 2487-) \tnum. features: 181)\n",
      "\tTP=154 \tTN=333 \tFP=12 \tFN=17\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Feature Set</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Recall</th>\n",
       "      <th>TrueNegRate</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>AllMeanSph</td>\n",
       "      <td>94.19</td>\n",
       "      <td>0.87</td>\n",
       "      <td>90.06</td>\n",
       "      <td>96.23</td>\n",
       "      <td>92.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Algorithm Feature Set  Accuracy   MCC  Recall  TrueNegRate  Precision\n",
       "0  ExtraTrees  AllMeanSph     94.19  0.87   90.06        96.23      92.22"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# specify algorithm and feature set for MAHOMES (top model from outer CV results)\n",
    "MAHOMES_alg = \"ExtraTrees\"\n",
    "MAHOMES_feature_set = \"AllMeanSph\"\n",
    "\n",
    "## set extra trees classifier with optimal parameters found during inner CV\n",
    "MAHOMES_clf = ExtraTreesClassifier(n_estimators=500, min_samples_split=3, max_depth=None,\n",
    "                           criterion=\"gini\",bootstrap=False, class_weight=None,\n",
    "                           max_features=None)\n",
    "\n",
    "\n",
    "MAHOMES_Tsite_predictions = evaluate_model_with_Tsite(MAHOMES_clf, MAHOMES_feature_set)\n",
    "\n",
    "## display final results\n",
    "scores = check_result_metrics(MAHOMES_alg, MAHOMES_feature_set,  MAHOMES_Tsite_predictions)\n",
    "display(scores.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
